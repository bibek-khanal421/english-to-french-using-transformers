{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'dense_dim': self.dense_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'sequence_length': self.sequence_length,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'embed_dim': self.embed_dim,\n",
    "        })\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Creates a layer from its config.\n",
    "        This method is the reverse of `get_config`,\n",
    "        capable of instantiating the same layer from the config\n",
    "        dictionary. It does not handle layer connectivity\n",
    "        (handled by Network), nor weights (handled by `set_weights`).\n",
    "        Arguments:\n",
    "            config: A Python dictionary, typically the\n",
    "                output of get_config.\n",
    "        Returns:\n",
    "            A layer instance.\n",
    "        \"\"\"\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(latent_dim, activation=\"relu\"), tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_vocab = np.load('imp/e_vocab.npy',allow_pickle=True)\n",
    "f_vocab = np.load('imp/f_vocab.npy',allow_pickle=True)\n",
    "X_eng = np.load('imp/etp.npy',allow_pickle=True)\n",
    "X_fr = np.load('imp/ftp.npy',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eng, X_test_eng,X_train_fr, X_test_fr = train_test_split(X_eng,X_fr, test_size=0.05, random_state=42)\n",
    "X_test_eng ,X_val_eng,X_test_fr ,X_val_fr = train_test_split(X_test_eng,X_test_fr, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 166608\n",
      "train label size 166608\n",
      "test dataset size: 2630\n",
      "test label size 2630\n",
      "val dataset size: 6139\n",
      "val label size 6139\n"
     ]
    }
   ],
   "source": [
    "print('train dataset size:',len(X_train_eng))\n",
    "print('train label size',len(X_train_fr))\n",
    "print('test dataset size:',len(X_test_eng))\n",
    "print('test label size',len(X_test_fr))\n",
    "print('val dataset size:',len(X_val_eng))\n",
    "print('val label size',len(X_val_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_e_train = X_train_eng\n",
    "X_f_train = X_train_fr[:, :-1]\n",
    "Y_train = X_train_fr[:, 1:]\n",
    "X_e_test = X_test_eng\n",
    "X_f_test = X_test_fr\n",
    "X_e_val = X_val_eng\n",
    "X_f_val = X_val_fr[:, :-1]\n",
    "Y_val = X_val_fr[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "english_sequence_length = len(X_eng[0])\n",
    "french_sequence_length = len(X_eng[0])\n",
    "english_vocab_size = len(e_vocab.item())\n",
    "french_vocab_size = len(f_vocab.item())\n",
    "\n",
    "\n",
    "encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(english_sequence_length, english_vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = tf.keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs =tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = tf.keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(french_sequence_length, french_vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = tf.keras.layers.Dense(french_vocab_size, activation=\"softmax\")(x)\n",
    "decoder = tf.keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = tf.keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding (Positiona (None, None, 256)    3731200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder (Transforme (None, None, 256)    3155456     positional_embedding[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, None, 27283)  19268499    decoder_inputs[0][0]             \n",
      "                                                                 transformer_encoder[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 26,155,155\n",
      "Trainable params: 26,155,155\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "epochs = 30  # This should be at least 30 for convergence\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=1.5e-5)\n",
    "transformer.compile(optimizer='rmsprop', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "transformer.summary()\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"transformer_new.h5\", save_best_only=True,save_weights_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",min_delta=0,patience=3,verbose=0,mode=\"auto\",baseline=None,restore_best_weights=False,)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "651/651 [==============================] - 237s 356ms/step - loss: 0.7066 - accuracy: 0.4620 - val_loss: 0.4986 - val_accuracy: 0.6010\n",
      "Epoch 2/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.4509 - accuracy: 0.6433 - val_loss: 0.3740 - val_accuracy: 0.6944\n",
      "Epoch 3/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.3719 - accuracy: 0.7034 - val_loss: 0.3308 - val_accuracy: 0.7295\n",
      "Epoch 4/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.3294 - accuracy: 0.7333 - val_loss: 0.3130 - val_accuracy: 0.7415\n",
      "Epoch 5/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.3053 - accuracy: 0.7524 - val_loss: 0.3017 - val_accuracy: 0.7518\n",
      "Epoch 6/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.2896 - accuracy: 0.7649 - val_loss: 0.2988 - val_accuracy: 0.7565\n",
      "Epoch 7/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.2780 - accuracy: 0.7749 - val_loss: 0.2901 - val_accuracy: 0.7616\n",
      "Epoch 8/30\n",
      "651/651 [==============================] - 231s 354ms/step - loss: 0.2691 - accuracy: 0.7825 - val_loss: 0.2881 - val_accuracy: 0.7668\n",
      "Epoch 9/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.2617 - accuracy: 0.7887 - val_loss: 0.2876 - val_accuracy: 0.7644\n",
      "Epoch 10/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.2557 - accuracy: 0.7944 - val_loss: 0.2838 - val_accuracy: 0.7699\n",
      "Epoch 11/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.2504 - accuracy: 0.7991 - val_loss: 0.2828 - val_accuracy: 0.7731\n",
      "Epoch 12/30\n",
      "651/651 [==============================] - 231s 355ms/step - loss: 0.2452 - accuracy: 0.8035 - val_loss: 0.2820 - val_accuracy: 0.7739\n",
      "Epoch 13/30\n",
      "651/651 [==============================] - 228s 350ms/step - loss: 0.2403 - accuracy: 0.8079 - val_loss: 0.2840 - val_accuracy: 0.7728\n",
      "Epoch 14/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2364 - accuracy: 0.8112 - val_loss: 0.2844 - val_accuracy: 0.7709\n",
      "Epoch 15/30\n",
      "651/651 [==============================] - 227s 348ms/step - loss: 0.2324 - accuracy: 0.8148 - val_loss: 0.2828 - val_accuracy: 0.7748\n",
      "Epoch 16/30\n",
      "651/651 [==============================] - 227s 348ms/step - loss: 0.2289 - accuracy: 0.8181 - val_loss: 0.2845 - val_accuracy: 0.7743\n",
      "Epoch 17/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2258 - accuracy: 0.8208 - val_loss: 0.2836 - val_accuracy: 0.7762\n",
      "Epoch 18/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2225 - accuracy: 0.8236 - val_loss: 0.2820 - val_accuracy: 0.7774\n",
      "Epoch 19/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2196 - accuracy: 0.8259 - val_loss: 0.2822 - val_accuracy: 0.7774\n",
      "Epoch 20/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2170 - accuracy: 0.8286 - val_loss: 0.2843 - val_accuracy: 0.7796\n",
      "Epoch 21/30\n",
      "651/651 [==============================] - 227s 348ms/step - loss: 0.2145 - accuracy: 0.8306 - val_loss: 0.2850 - val_accuracy: 0.7802\n",
      "Epoch 22/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2121 - accuracy: 0.8329 - val_loss: 0.2830 - val_accuracy: 0.7806\n",
      "Epoch 23/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2093 - accuracy: 0.8352 - val_loss: 0.2905 - val_accuracy: 0.7733\n",
      "Epoch 24/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2074 - accuracy: 0.8367 - val_loss: 0.2835 - val_accuracy: 0.7793\n",
      "Epoch 25/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2050 - accuracy: 0.8393 - val_loss: 0.2838 - val_accuracy: 0.7818\n",
      "Epoch 26/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2030 - accuracy: 0.8407 - val_loss: 0.2851 - val_accuracy: 0.7821\n",
      "Epoch 27/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.2009 - accuracy: 0.8425 - val_loss: 0.2847 - val_accuracy: 0.7828\n",
      "Epoch 28/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.1990 - accuracy: 0.8441 - val_loss: 0.2894 - val_accuracy: 0.7813\n",
      "Epoch 29/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.1974 - accuracy: 0.8452 - val_loss: 0.2900 - val_accuracy: 0.7816\n",
      "Epoch 30/30\n",
      "651/651 [==============================] - 226s 348ms/step - loss: 0.1957 - accuracy: 0.8469 - val_loss: 0.2879 - val_accuracy: 0.7814\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit((X_e_train,X_f_train),Y_train, shuffle=True,epochs=epochs,batch_size=256,validation_data=((X_e_val,X_f_val),Y_val),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(text,vocab):\n",
    "    temp_text = []\n",
    "    temp_array = []\n",
    "    for word in text:\n",
    "        try:\n",
    "            temp_array.append(vocab[str(word)])\n",
    "        except:\n",
    "            temp_array.append(vocab['<unk>'])\n",
    "    temp_text.append(temp_array)\n",
    "    return temp_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:  <bos> i'm looking for a room for rent . <eos>\n",
      "french: <bos> je cherche une chambre pour loyer . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> she's unfit for the job . <eos>\n",
      "french: <bos> elle est en relation pour le poste . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> this can't wait until tomorrow . <eos>\n",
      "french: <bos> cela ne peut pas attendre jusqu'à demain . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> i bought a watch . <eos>\n",
      "french: <bos> j'ai acheté une montre . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> no one was helping us . <eos>\n",
      "french: <bos> personne ne nous aide . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> my eyes are blue . <eos>\n",
      "french: <bos> mes yeux sont jaunes . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> tom exaggerates . <eos>\n",
      "french: <bos> tom a des soins . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> everyone got off the bus . <eos>\n",
      "french: <bos> tout le monde a passé le bus . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> now that he is old , it is your duty to go look after him . <eos>\n",
      "french: <bos> maintenant qu'il est vieux , c'est ton devoir de faire ça va le prendre soin de lui . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> i have a daughter . <eos>\n",
      "french: <bos> j'ai une fille . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> i'm not sure why they asked me that . <eos>\n",
      "french: <bos> je ne suis pas sûre de la chose qui m'a demandé cela . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> why did you go over my head on this ? <eos>\n",
      "french: <bos> pourquoi avez - vous passé la tête cela ? <eos>\n",
      "\n",
      "\n",
      "english:  <bos> can you teach me to fly ? <eos>\n",
      "french: <bos> pouvez - vous m'apprendre à voler ? <eos>\n",
      "\n",
      "\n",
      "english:  <bos> i don't mind that at all . <eos>\n",
      "french: <bos> je n'en ai pas le moins du monde . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> you won't be harmed . <eos>\n",
      "french: <bos> tu ne te feras pas de mal . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> i will show you around the city . <eos>\n",
      "french: <bos> je vais te montrer les hommes . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> are they now available throughout switzerland ? <eos>\n",
      "french: <bos> sont - ils maintenant disponibles en afrique ? <eos>\n",
      "\n",
      "\n",
      "english:  <bos> she is bad - mannered . <eos>\n",
      "french: <bos> elle a de mauvais - t - même . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> the farmer plowed his field all day . <eos>\n",
      "french: <bos> le docteur vous dérange cet terme toute la journée . <eos>\n",
      "\n",
      "\n",
      "english:  <bos> i can't buy you that dress . <eos>\n",
      "french: <bos> je ne peux pas vous acheter cette robe . <eos>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spa_index_lookup = np.load('imp/f_vocab_rev.npy',allow_pickle=True)\n",
    "max_decoded_sentence_length = 50\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = input_sentence\n",
    "    decoded_sentence = \"<bos>\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = token(decoded_sentence.split(' '),f_vocab.item())\n",
    "        tokenized_target_sentence = tf.keras.preprocessing.sequence.pad_sequences(tokenized_target_sentence, maxlen=50, padding=\"post\")\n",
    "        predictions = transformer([tokenized_input_sentence.reshape(1,50), tokenized_target_sentence.reshape(1,50)])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"<eos>\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "eng_index_lookup = np.load('imp/e_vocab_rev.npy',allow_pickle=True)\n",
    "max_decoded_sentence_length = 50\n",
    "def decode_english(input_seq):\n",
    "    decoded_sentence = \"\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        sampled_token = eng_index_lookup[input_seq[i]]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"<eos>\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    value = np.random.choice(np.arange(0,len(X_test_eng)))\n",
    "    translated = decode_sequence(X_e_test[value])\n",
    "    actual = decode_english(X_e_test[value])\n",
    "    print('english:',actual)\n",
    "    print('french:',translated)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Spleeter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d347aeb3a5a0835525c42ff54dfd12f4da455788e0882309fa4eec38c6aaa9ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
