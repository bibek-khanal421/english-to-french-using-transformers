{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "from tqdm import tqdm \n",
    "import re\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data \n",
    "file = pd.read_csv(r\"D:\\en-french\\archive0\\eng-french.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distributing the languages\n",
    "english = file['English words/sentences']\n",
    "french = file['French words/sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total english words:',len(english))\n",
    "print('total french words:',len(french))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the data \n",
    "def edit(text_eng,text_fre,punc):\n",
    "    eng_sent = \"\"\n",
    "    for i,char in enumerate(text_eng.lower()):\n",
    "        if char in ('~', ':', '+', '[', '\\\\', '@', '^', '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '`', '}', '.', '_', '=', ']', '!', '>', ';', '?', '#', '$', ')', '/') and i>0:\n",
    "            if punc:\n",
    "                eng_sent += ' '\n",
    "                eng_sent += char\n",
    "                eng_sent += ' '\n",
    "            else:\n",
    "                eng_sent += ' '\n",
    "        else:\n",
    "            eng_sent += char\n",
    "    eng_sent = eng_sent.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    eng_sent = re.sub(\"\\(\",\" ( \",eng_sent)\n",
    "    eng_sent = re.sub(\"\\s\\s+\", \" \", eng_sent)\n",
    "\n",
    "    \n",
    "    fre_sent = \"\"\n",
    "    for i,char in enumerate(text_fre.lower()):\n",
    "        if char in ('~', ':', '+', '[', '\\\\', '@', '^', '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '}', '.', '_', '=', ']', '!', '>', ';', '?', '#', '$', ')', '/') and i>0:\n",
    "            if punc:\n",
    "                fre_sent += ' '\n",
    "                fre_sent += char\n",
    "                fre_sent += ' '\n",
    "            else:\n",
    "                fre_sent += ' '\n",
    "        else:\n",
    "            fre_sent += char\n",
    "    # fre_sent = re.sub(\",\",\" , \",fre_sent)\n",
    "    fre_sent = fre_sent.replace('\\u202f', ' ').replace('\\xa0', ' ')\n",
    "    fre_sent = re.sub(\"\\(\",\" ( \",fre_sent)\n",
    "    fre_sent = re.sub(\"\\s\\s+\",\" \", fre_sent)\n",
    "    return eng_sent,fre_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = True\n",
    "E=[]\n",
    "F=[]\n",
    "for i in tqdm(range(len(english))):\n",
    "    english_data,french_data = edit(english[i],french[i],punc)\n",
    "    english_data = english_data.split(' ')\n",
    "    french_data = french_data.split(' ')\n",
    "    try:\n",
    "        english_data.remove('')\n",
    "        french_data.remove('')\n",
    "    except:\n",
    "        continue\n",
    "    E.append(english_data)\n",
    "    F.append(french_data)\n",
    "E = np.array(E, dtype=object)\n",
    "F = np.array(F, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the largest sentence length for both languages \n",
    "c = []\n",
    "def sent_len(lang):\n",
    "    for list in lang:\n",
    "        c.append(len(list))\n",
    "    return np.array(c).max()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_e = sent_len(E)\n",
    "max_f = sent_len(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('longest sentence for english corpus:',max_e)\n",
    "print('longest sentence for french corpus:',max_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the <bos> and <eos> to all sentences \n",
    "def add_pad(lang):\n",
    "    temp = []\n",
    "    for list in lang:\n",
    "        list.insert(0,'<bos>')\n",
    "        list.append('<eos>')\n",
    "        temp.append(list)\n",
    "    return np.array(temp, dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = add_pad(E)\n",
    "F = add_pad(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab = d2l.Vocab(E, min_freq=1, reserved_tokens=['<bos>', '<eos>','<pad>'])\n",
    "fre_vocab = d2l.Vocab(F, min_freq=1, reserved_tokens=['<bos>', '<eos>','<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of the dataset \n",
    "def token(text,vocab):\n",
    "    temp_text = []\n",
    "    for array in tqdm(text):\n",
    "        temp_array = []\n",
    "        for word in array:\n",
    "            try:\n",
    "                temp_array.append(vocab[word])\n",
    "            except:\n",
    "                temp_array.append(vocab['<unk>'])\n",
    "        temp_text.append(temp_array)\n",
    "    return temp_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_token = token(E,eng_vocab.token_to_idx)\n",
    "F_token = token(F,fre_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the tokenized text \n",
    "padded_english = tf.keras.preprocessing.sequence.pad_sequences(E_token,maxlen=50, padding=\"post\")\n",
    "print(len(padded_english[2]))\n",
    "padded_french = tf.keras.preprocessing.sequence.pad_sequences(F_token, maxlen=50+1, padding=\"post\")\n",
    "print(len(padded_french[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the tokenized padded text and vocabularies\n",
    "# saving english vocabulary\n",
    "np.save('imp/e_vocab.npy',eng_vocab.token_to_idx,allow_pickle=True)\n",
    "np.save('imp/e_vocab_rev.npy',eng_vocab.idx_to_token,allow_pickle=True)\n",
    "# saving the french vocabulary \n",
    "np.save('imp/f_vocab.npy',fre_vocab.token_to_idx,allow_pickle=True)\n",
    "np.save('imp/f_vocab_rev.npy',fre_vocab.idx_to_token,allow_pickle=True)\n",
    "# saving the tokenied padded dataset \n",
    "np.save('imp/etp.npy',padded_english,allow_pickle=True)\n",
    "np.save('imp/ftp.npy',padded_french,allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Spleeter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d347aeb3a5a0835525c42ff54dfd12f4da455788e0882309fa4eec38c6aaa9ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
